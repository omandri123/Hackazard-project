![github-submission-banner](https://github.com/user-attachments/assets/a1493b84-e4e2-456e-a791-ce35ee2bcf2f)

# üöÄ TAS- Training Agent System

>TAS (Training Agent System) is an advanced AI-powered platform designed for both interview preparation and real-time candidate evaluation. Whether you're a job seeker looking to sharpen your skills or a company aiming to assess candidates with precision, TAS has you covered. It analyzes facial expressions, voice tone, speech content, and body language to provide detailed feedback or evaluation ‚Äî making it the perfect tool for both personal growth and professional hiring.

---

## üìå Problem Statement 1 ‚Äì Weave AI magic with Groq

---

## üéØ Objective

We can prepare for exams for years using books and lectures.
But what about interviews?

An interview is not just a test of knowledge‚Äîit‚Äôs a test of presence, communication, confidence, and behavior. These are physical and psychological traits you can‚Äôt just read about. You need real-time practice and feedback.
That‚Äôs where TAS ‚Äì The Training Agent System comes in.

TAS is built around two major objectives:

1. Lack of Constructive Feedback: After most interviews, candidates are either placed or rejected without knowing why. There's no meaningful feedback that helps them learn or grow from the experience.


2. No Dedicated and Optimized Training Tool for interview training: While there are platforms for interview prep, but they all lack real-time analysis and detailed, personalized feedback. Most of these softwares just judge based on your text response.

---

Solutions provided by TAS:

Solution to Objective 1: Training Agent Mode
This mode is designed for real interviews. The HR or interviewer can launch TAS in Training Agent Mode, enter the candidate's name, and set the judgment criteria. TAS then automatically accesses the system‚Äôs camera and microphone to start analyzing the candidate‚Äôs posture, facial expressions, and speech in real-time.
Once the interview ends, the HR simply stops the session. All collected data is processed and sent to GROQ AI, which generates a comprehensive feedback report‚Äîcomplete with performance scores, behavioral analysis, graphs, and improvement suggestions. This detailed PDF report can then be handed over to the candidate as a learning resource.

Solution to Objective 2: Interview Training Mode
This mode is for self-practice and preparation. The candidate can launch Interview Training Mode, enter their name and the purpose of the interview (e.g., job role, company, domain), and TAS will fetch a tailored set of interview questions generated by GROQ AI.
As the candidate answers each question, TAS analyzes facial expressions, posture, voice tone, and speech quality. After the session, it generates a detailed performance report in PDF format‚Äîincluding rankings, graphical summaries, and actionable feedback‚Äîhelping candidates track their growth and focus on areas that need improvement.


## üß† Team & Approach

### Team Name:  
`IET-SAG`

### Team Members:  
- Om Andri (github username - omandri123 / LinkedIn - Om Andri/ / Role : Developer)  
- Pintu Kumar (github username - PINTU1318)
- 
  *Insta ID (Om Andri) = https://www.instagram.com/__om__andri__/   Insta ID (Pintu Kumar) = https://www.instagram.com/pintu_rajput03/*

### Your Approach:  
- We chose this problem because interview performance is one of the most critical factors in personal and professional growth ‚Äî yet most people don‚Äôt get objective, actionable feedback on how they perform. At the same time, companies struggle to evaluate soft skills and behavioral traits effectively during interviews. TAS bridges this gap by combining facial expression analysis, speech processing, and intelligent feedback generation.
  
- One of the key challenges we addressed was real-time multi-modal data integration ‚Äî capturing and analyzing facial cues, tone, and speech simultaneously without lag. Ensuring smooth synchronization between modules and delivering instant feedback required careful handling of processing pipelines and resource management.
  
- During the development, we had several breakthroughs. One was integrating a live self-view window during practice sessions, replicating the "mirror effect" to help candidates self-correct in real time. Another was designing the system to seamlessly switch between Training Mode and Evaluation Mode, making it useful for both individuals and organizations. We also pivoted from traditional feedback to a detailed PDF report with visual insights like graphs and pie charts, enhancing clarity and user impact.

---

## üõ†Ô∏è Tech Stack

### Core Technologies Used:
- Frontend: Tkinter (for GUI), OpenCV (for live camera feed and emotion tracking)
- Backend: Python (for logic handling, audio & facial analysis), DeepFace, SpeechRecognition
- Database: SQLite (for storing user data, interview logs, and session reports)
- APIs: Groq API (for advanced AI analysis and feedback generation), Text-to-Speech (gTTS or pyttsx3)
- Hosting: Local system for offline training, optional cloud integration for enterprise-level deployment

### Sponsor Technologies Used (if any):
- [‚úÖ] **Groq:** _How you used Groq_  
- [ ] **Monad:** _Your blockchain implementation_  
- [ ] **Fluvio:** _Real-time data handling_  
- [ ] **Base:** _AgentKit / OnchainKit / Smart Wallet usage_  
- [ ] **Screenpipe:** _Screen-based analytics or workflows_  
- [ ] **Stellar:** _Payments, identity, or token usage_

---

## ‚ú® Key Features

Highlight the most important features of your project:

- ‚úÖ Dual Mode Functionality ‚Äì Includes both Interview Training Mode for candidates and Evaluation Mode for companies 
- ‚úÖ Real-Time Facial and Speech Analysis ‚Äì Monitors expressions, body posture, voice tone, and answer quality simultaneously
- ‚úÖ Smart Question Generation ‚Äì Personalized interview questions based on user goals and evaluation criteria
- ‚úÖ Detailed Feedback Report ‚Äì Generates a comprehensive PDF with graphs, pie charts, rankings, and improvement suggestions  

Add images, GIFs, or screenshots if helpful!

---

## üìΩÔ∏è Demo & Deliverables

- **Demo Video Link:** [YouTube Link: ](https://youtu.be/k36mRPnrYK8) ]  
- **Document Link (OneDrive):** [https://1drv.ms/w/c/b03b195cf5baf088/EZ64unxKhN5NjUSeMrbawAwBfaZz069cfn8prrg38ENqeg?e=OcftNR]  

---

## ‚úÖ Tasks & Bonus Checklist

- [‚úÖ] **All members of the team completed the mandatory task - Followed at least 2 of our social channels and filled the form** (Details in Participant Manual)  
- [‚úÖ] **All members of the team completed Bonus Task 1 - Sharing of Badges and filled the form (2 points)**  (Details in Participant Manual)
- [‚úÖ] **All members of the team completed Bonus Task 2 - Signing up for Sprint.dev and filled the form (3 points)**  (Details in Participant Manual)

---

## üß™ How to Run the Project

### Requirements:
- Python 3.8+
- Pip (Python package manager)
- OpenCV, Tkinter, DeepFace, SpeechRecognition
- Groq API key (optional, for AI-based feedback)
- .env file (for storing API keys securely, if needed)

### Local Setup:
```bash
# Clone the repository
git clone https://github.com/omandri123/TAS-Training-Agent-System-HACKHAZARDS-PROJECT
cd TAS-Training-Agent-System-HACKHAZARDS-PROJECT

# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate

# Install required dependencies
pip install -r requirements.txt

# Run the main application
python main.py

```

Provide any backend/frontend split or environment setup notes here.

---

## üß¨ Future Scope

List of improvements, extensions, or follow-up features planned for TAS:

- More Integrations ‚Äì Integration with platforms like Zoom, MS Teams, or job portals for seamless real-world application
- Security Enhancements ‚Äì Improved data encryption, secure login system, and GDPR-compliant data handling
- Localization & Accessibility ‚Äì Multilingual support, screen reader compatibility, and adaptive design for wider reach
- AI-Powered Question Adaptation ‚Äì Real-time adjustment of difficulty based on candidate performance
- Cloud Sync & Analytics Dashboard ‚Äì Centralized storage with admin dashboards for organizations to track multiple candidates
- Plugin System ‚Äì Allow third-party modules for HR-specific customization 

---

## üìé Resources / Credits

- APIs Used:
  - Groq AI ‚Äì For intelligent feedback generation
  - Google Speech Recognition API ‚Äì For voice-to-text processing

- Open Source Libraries & Tools:
  - OpenCV ‚Äì For real-time facial expression analysis
  - DeepFace ‚Äì For emotion detection from facial features
  - Tkinter ‚Äì For GUI development
  - Matplotlib ‚Äì For generating graphs and pie charts in reports
  - fpdf ‚Äì For creating PDF feedback reports

- Acknowledgements:
  - Special thanks to the open-source community for providing powerful tools and libraries
  - Gratitude to all mentors, testers, and early users who provided feedback during development 

---

## üèÅ Final Words

Building TAS has been an incredible journey of innovation, problem-solving, and pushing technical boundaries.
From sleepless debugging nights to those "aha!" moments when everything finally worked ‚Äî this project tested both our skills and patience.
We tackled challenges in real-time data processing, synchronized multiple modules, and crafted a user-friendly system from the ground up. 
Collaborating, learning new libraries, and integrating AI feedback felt both exciting and empowering.

Big shout-out to our team, mentors, and the hackathon organizers for creating an inspiring space to build something meaningful. 
TAS is just the beginning ‚Äî we‚Äôre excited to see how far we can take it! üöÄ

---
